SAM 2:
	- Choosed first SAM 2 model to test;
	- Initial tests were produced using GPU Nvidia RTX 2060;
	- Setup of SAM 2, resolution of errors and conflits. First tried to install SAM 2 on windows with no success. It was then decided to install on WSL. After a deep study of wsl, python and environments, it was decided to install using conda, creating a dedicated environment with the necessary dependencies;

	- A SAM 2 image predictor model checkpoint was tested with sugested images to learn the process of SAM 2;
	- The same image predictor model checkpoint was used to test on multiphase flow images ( geom1_qc2_0001 - add image to report). It was donne an interactive segmentation refinement process on the model checkpoint which consisted in selecting 10 points and label them has part of segmentation or part of background. The annotation process was used with help of LabelMe (https://github.com/wkentaro/labelme), also with some problems during the setup, for example it was not able to install on windows, and it was some needed configurations in wsl to make it able to work, in an also specific conda environment. It was donne 3 iterations of this process and each time it was used on the last refined model. In each iteration it was able to select one of three masks with a respective score. The best masks were not the ones with the best score in this case since the ones with best score segmentated more general features including all the pipe with the bubbles, and the one with less score segmentated more specific bubbles related to the labels but not all of the existant bubbles in the image.

	- It was then tested the SAM 2 video predictor with the same model checkpoint for a sugested video to learn the process of video segmentation.
	- In the first runs of the program, the propagation of the video for the development of masklet was slow, taking 4 hours to propagate half of a video of 6 seconds, with 200 frame images. This was due to some probable configuration of the machine. After restarting and reseting the program, the propagation phase was a lot faster, taking 4 minutes to propagate all the 200 frames of the video and building the masklet.
	- It was selected the video "qc2_qd1_10x_formacao.avi" with 16 seconds and 600 MB of size to test for multiphase flow images. The video was converted into frames using a tool called ffmpeg. This tool was installed inside the dedicated python environment, and used the command line to run and generate the frames into a destination directory, so then to be used in the notebook. It was created 493 frame images, stored inside the folder.
	- After some experiments, it was noticed that the first time a video prediction started running, it took a huge ammount of time, and the subsquent runs of the same prediction were a lot faster compared with the first. Everytime the model was run with a diferent annotation, it reseted and again took huge time in first run. This was proved with the example that the video with 500 frames took 10 hours to propagate and produce the masklet, with about 60 to 70 seconds per iteration/frame, and the subsquent runs took less than 1 hour with 3 to 4 seconds per iteration/frame. The video before with 6 seconds was similiar, initially took about 20 seconds per iteration, and the subsquent runs about 1 second per iteration. The faster ratio is likely due to the resolution and size of the images, the ones with multiphase having significantly higher resolution and size. The explanation for the subsquent runs being faster than the first ones are due to:
		- The first time you run a deep learning model like SAM-2, PyTorch and CUDA may need to compile and optimize the GPU kernels.
		- This can take a long time (hours in some cases), especially if itâ€™s the first time you're running it on your GPU.
		- Once compiled, these optimized kernels are cached, making future runs significantly faster.